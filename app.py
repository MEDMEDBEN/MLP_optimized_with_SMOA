# app.py - Streamlit : affichage + comparaison MLP seul vs MLP + SMOA
import streamlit as st
import pandas as pd
import json
import os
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image

# ======================== CONFIGURATION PAGE ========================
st.set_page_config(
    page_title="Classification - MLP vs MLP + SMOA",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ======================== CSS MODERNE ========================
st.markdown("""
    <style>
    /* Page principale */
    .main {background-color: #f8f9fa;}
    .stApp {background-image: linear-gradient(135deg, #667eea 0%, #764ba2 100%);}
    
    /* Sidebar - Design moderne et √©l√©gant */
    .sidebar .sidebar-content {
        background: linear-gradient(180deg, #2c3e50 0%, #34495e 100%);
        padding: 30px 20px;
    }
    
    /* Titre de la sidebar */
    .sidebar .sidebar-content h1 {
        color: #ecf0f1;
        text-align: center;
        font-size: 24px;
        margin-bottom: 30px;
        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        border-bottom: 3px solid #667eea;
        padding-bottom: 15px;
    }
    
    /* Boutons radio (menu) */
    .sidebar .sidebar-content label {
        color: #ecf0f1;
        font-size: 16px;
        font-weight: 500;
        padding: 12px 15px;
        margin: 8px 0;
        border-radius: 8px;
        transition: all 0.3s ease;
        cursor: pointer;
        display: block;
        border-left: 4px solid transparent;
    }
    
    .sidebar .sidebar-content label:hover {
        background-color: rgba(102, 126, 234, 0.2);
        border-left: 4px solid #667eea;
        padding-left: 20px;
    }
    
    /* Boutons s√©lectionn√©s */
    .sidebar input[type="radio"]:checked + span {
        color: #667eea;
        font-weight: bold;
    }
    
    /* Titres et sous-titres */
    h1, h2, h3 {color: #2c3e50;}
    
    /* Boutons */
    .stButton>button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 8px;
        padding: 12px 24px;
        font-size: 16px;
        border: none;
        transition: all 0.3s ease;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
    }
    
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(102, 126, 234, 0.5);
    }
    
    /* Bo√Ætes m√©triques */
    .metric-box {
        background: white;
        padding: 20px;
        border-radius: 12px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        text-align: center;
    }
    
    /* Expanders */
    .streamlit-expanderHeader {
        background-color: #f0f2f6 !important;
        border-radius: 8px !important;
        font-weight: 600 !important;
    }
    
    /* Divider */
    hr {
        border-color: #667eea;
        margin: 20px 0;
    }
    </style>
""", unsafe_allow_html=True)

# ======================== CHEMINS (√† adapter si besoin) ========================
# MLP + SMOA
SMOA_MODEL_PATH     = "results_smoa/best_model_smoa.pkl"
SMOA_METRICS_PATH   = "results_smoa/best_hyperparameters.json"
SMOA_REPORT_PATH    = "results_smoa/classification_report_smoa.txt"
SMOA_GRAPH_PATH     = "results_smoa/graphs_summary_smoa.png"

# MLP seul (baseline)
MLP_MODEL_PATH      = "results_mlp/mlp_baseline.pkl"
MLP_REPORT_PATH     = "results_mlp/classification_report_mlp.txt"
MLP_GRAPH_PATH      = "results_mlp/accuracy_mlp.png"

# Helpers
def render_results(title, base_dir):
    st.subheader(title)
    if not os.path.exists(base_dir):
        st.info(f"Dossier non trouv√© : {base_dir}")
        return

    found = False
    for root, _, files in os.walk(base_dir):
        files = sorted(files)
        for fname in files:
            found = True
            fpath = os.path.join(root, fname)
            rel = os.path.relpath(fpath, base_dir)
            st.markdown(f"**Fichier : {rel}**")
            lower = fname.lower()
            try:
                if lower.endswith((".png", ".jpg", ".jpeg", ".gif")):
                    st.image(fpath, use_container_width=True)
                elif lower.endswith(".txt"):
                    with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
                        content = f.read()
                    st.text_area("Contenu", content, height=260, key=f"txt-{title}-{rel}")
                elif lower.endswith(".json"):
                    with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
                        data = json.load(f)
                    st.json(data)
                elif lower.endswith(".pkl"):
                    st.info(f"Fichier mod√®le trouv√© : {rel}")
                else:
                    st.write(f"Fichier disponible : {rel}")
            except Exception as e:
                st.error(f"Erreur lors de l'affichage de {rel} : {e}")

    if not found:
        st.info("Aucun fichier trouv√© dans ce dossier.")


def extract_accuracy(report_path):
    try:
        if not os.path.exists(report_path):
            return None
        with open(report_path, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                if "accuracy" in line.lower():
                    parts = line.strip().split()
                    for p in parts:
                        try:
                            val = float(p)
                            if 0 <= val <= 1:
                                return val
                        except ValueError:
                            continue
        return None
    except Exception:
        return None


def parse_classification_report(report_path):
    """Extrait les informations cl√©s du rapport de classification"""
    try:
        with open(report_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()
        return content
    except Exception:
        return None

# Ton tableau de donn√©es
DATA_PATH = "data/covtype.csv"   # ‚Üê CHANGE √áA POUR TON FICHIER

# Nom de la dataset (tu peux changer)
DATASET_NAME = "Mon Tableau de Donn√©es (lignes √ó colonnes)"  # ‚Üê CHANGE √áA

# ======================== SIDEBAR ========================
st.sidebar.markdown("""
<div style="text-align: center; padding: 20px 0;">
    <h1 style="color: #667eea; font-size: 32px; margin: 0;">üå≤ STATS Dashboard</h1>
    <p style="color: #95a5a6; font-size: 12px; margin: 10px 0 0 0;">Forest Cover Classification</p>
</div>
""", unsafe_allow_html=True)

st.sidebar.markdown("---")

st.sidebar.markdown("""
<style>
.sidebar-menu {
    display: flex;
    flex-direction: column;
    gap: 10px;
}
.menu-item {
    padding: 15px;
    border-radius: 8px;
    text-align: center;
    font-weight: 600;
    font-size: 14px;
}
</style>
""", unsafe_allow_html=True)

page = st.sidebar.radio(
    "üìç Navigation",
    [
        "üå≤ Donn√©es (EDA)",
        "üìä Comparaison",
        "ü§ñ Pr√©diction",
        "üß¨ Algo"
    ],
    label_visibility="collapsed"
)

# Mapper les options affich√©es aux cl√©s utilis√©es
page_map = {
    "üå≤ Donn√©es (EDA)": "Donn√©es (EDA)",
    "üìä Comparaison": "Comparaison des mod√®les",
    "ü§ñ Pr√©diction": "Pr√©diction (SMOA)",
    "üß¨ Algo": "Algo (explications)"
}
page = page_map.get(page, page)

st.sidebar.markdown("---")
st.sidebar.markdown("""
<div style="text-align: center; font-size: 12px; color: #95a5a6; margin-top: 30px;">
    <p>üöÄ <b>v1.0</b> | ML Classification App</p>
    <p>Powered by Streamlit & scikit-learn</p>
</div>
""", unsafe_allow_html=True)

# ======================== ACCUEIL ========================
if page == "Donn√©es (EDA)":
    st.title("üå≤ Donn√©es et EDA")
    st.markdown("""
    ### üå≥ Forest Cover Type (Covertype)
    
    Le jeu de donn√©es **Forest Cover Type (Covertype)** d√©crit des parcelles du **Roosevelt National Forest** 
    (Colorado, USA) et vise √† pr√©dire le type de couvert forestier dominant (7 classes) √† partir de variables 
    topographiques, de distances √† certaines structures et de caract√©ristiques de sol et de zones sauvages. 
    
    Chaque observation correspond √† une cellule de grille de **30 m √ó 30 m** √©chantillonn√©e sur quatre zones 
    sauvages distinctes.
    """)

    if os.path.exists(DATA_PATH):
        try:
            if DATA_PATH.lower().endswith('.csv'):
                df = pd.read_csv(DATA_PATH)
            elif DATA_PATH.lower().endswith(('.xlsx', '.xls')):
                df = pd.read_excel(DATA_PATH)
            else:
                st.error("Format non support√© (CSV ou Excel uniquement).")
                df = None
        except Exception as e:
            st.error(f"Erreur lors du chargement des donn√©es: {e}")
            df = None

        if df is not None:
            st.success(f"üìä Dimensions : {df.shape[0]:,} lignes √ó {df.shape[1]} colonnes")

            # Aper√ßu des donn√©es EN PREMIER
            st.subheader("üëÄ Aper√ßu des donn√©es")
            st.dataframe(df.head(50), use_container_width=True)
            
            # T√©l√©chargement
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button("‚¨áÔ∏è T√©l√©charger le tableau complet (CSV)", csv, "dataset_complet.csv", "text/csv")

            # Colonnes & Types + Valeurs manquantes (c√¥te √† c√¥te)
            colA, colB = st.columns(2)
            with colA:
                st.subheader("üìã Colonnes & Types")
                types_df = pd.DataFrame({
                    "colonne": df.columns,
                    "type": [str(t) for t in df.dtypes]
                })
                st.dataframe(types_df, use_container_width=True)

            with colB:
                st.subheader("‚ö†Ô∏è Valeurs manquantes")
                na_df = df.isna().sum().reset_index()
                na_df.columns = ["colonne", "manquants"]
                st.dataframe(na_df, use_container_width=True)

            # Statistiques descriptives
            st.subheader("üìà Statistiques descriptives (num√©riques)")
            desc = df.select_dtypes(include=['number']).describe().T
            st.dataframe(desc, use_container_width=True)

            # Corr√©lation
            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
            if len(numeric_cols) >= 2:
                st.subheader("üîó Matrice de corr√©lation (10 premi√®res colonnes num√©riques)")
                # Prendre seulement les 10 premi√®res colonnes num√©riques
                top_cols = numeric_cols[:10]
                corr = df[top_cols].corr()
                fig, ax = plt.subplots(figsize=(12, 10))
                sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", 
                            cbar_kws={"label": "Corr√©lation"}, ax=ax, 
                            square=True, linewidths=0.5, vmin=-1, vmax=1)
                ax.set_title("Matrice de Corr√©lation", fontsize=14, fontweight='bold', pad=20)
                plt.xticks(rotation=45, ha='right', fontsize=10)
                plt.yticks(rotation=0, fontsize=10)
                plt.tight_layout()
                st.pyplot(fig, use_container_width=True)
                plt.close()

            # Distribution simple d'une colonne num√©rique
            st.subheader("üìä Distribution d'une colonne")
            col_for_dist = st.selectbox("üìç Choisir une colonne pour la distribution", df.columns.tolist())
            if col_for_dist:
                fig2, ax2 = plt.subplots(figsize=(10, 5))
                if pd.api.types.is_numeric_dtype(df[col_for_dist]):
                    ax2.hist(df[col_for_dist].dropna(), bins=30, color="#667eea", edgecolor='black')
                    ax2.set_title(f"Histogramme - {col_for_dist}", fontsize=14, fontweight='bold')
                    ax2.set_xlabel("Valeurs", fontsize=12)
                    ax2.set_ylabel("Fr√©quence", fontsize=12)
                else:
                    df[col_for_dist].value_counts().plot(kind='bar', ax=ax2, color="#764ba2", edgecolor='black')
                    ax2.set_title(f"Comptes par cat√©gorie - {col_for_dist}", fontsize=14, fontweight='bold')
                    ax2.set_xlabel("Cat√©gories", fontsize=12)
                    ax2.set_ylabel("Fr√©quence", fontsize=12)
                plt.tight_layout()
                st.pyplot(fig2)
                plt.close()
    else:
        st.warning(f"Fichier non trouv√© : {DATA_PATH}")
        st.info("Place ton fichier dans le dossier 'data/' et modifie le chemin dans le code.")

# ======================== COMPARAISON DES MOD√àLES ========================
elif page == "Comparaison des mod√®les":
    st.title("Comparaison : MLP seul vs MLP + SMOA")
    
    # Afficher les chemins pour le debug
    with st.expander("üîç Chemins des fichiers (debug)"):
        st.info(f"""
        **Chemins recherch√©s :**
        - MLP Rapport : `{MLP_REPORT_PATH}` ‚Üí {'‚úÖ Trouv√©' if os.path.exists(MLP_REPORT_PATH) else '‚ùå Manquant'}
        - MLP Graph : `{MLP_GRAPH_PATH}` ‚Üí {'‚úÖ Trouv√©' if os.path.exists(MLP_GRAPH_PATH) else '‚ùå Manquant'}
        - SMOA Rapport : `{SMOA_REPORT_PATH}` ‚Üí {'‚úÖ Trouv√©' if os.path.exists(SMOA_REPORT_PATH) else '‚ùå Manquant'}
        - SMOA Graph : `{SMOA_GRAPH_PATH}` ‚Üí {'‚úÖ Trouv√©' if os.path.exists(SMOA_GRAPH_PATH) else '‚ùå Manquant'}
        - SMOA Metrics : `{SMOA_METRICS_PATH}` ‚Üí {'‚úÖ Trouv√©' if os.path.exists(SMOA_METRICS_PATH) else '‚ùå Manquant'}
        
        **R√©pertoire courant :** `{os.getcwd()}`
        """)
    
    acc_mlp = extract_accuracy(MLP_REPORT_PATH)
    acc_smoa = extract_accuracy(SMOA_REPORT_PATH)

    # M√©triques principales c√¥te √† c√¥te
    colA, colB = st.columns(2)
    with colA:
        st.subheader("üß† MLP seul (baseline)")
        st.metric("Accuracy", f"{acc_mlp*100:.2f} %" if acc_mlp is not None else "N/A")
    with colB:
        st.subheader("üî¨ MLP + SMOA (optimis√©)")
        st.metric("Accuracy", f"{acc_smoa*100:.2f} %" if acc_smoa is not None else "N/A")

    # Rapports d√©taill√©s - ZONE COMPL√àTE
    st.markdown("---")
    st.subheader("üìä Rapports de Classification D√©taill√©s")
    
    col_rep1, col_rep2 = st.columns(2)
    
    with col_rep1:
        st.markdown("### **MLP (Baseline)**")
        if os.path.exists(MLP_REPORT_PATH):
            report_mlp = parse_classification_report(MLP_REPORT_PATH)
            if report_mlp:
                with st.container():
                    st.markdown("""
                    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                                padding: 20px; border-radius: 10px; color: white;">
                    <h4 style="margin-top: 0;">R√©sultats du mod√®le MLP</h4>
                    </div>
                    """, unsafe_allow_html=True)
                    st.text(report_mlp)
        else:
            st.info("Rapport MLP non trouv√©")
    
    with col_rep2:
        st.markdown("### **MLP + SMOA (Optimis√©)**")
        if os.path.exists(SMOA_REPORT_PATH):
            report_smoa = parse_classification_report(SMOA_REPORT_PATH)
            if report_smoa:
                with st.container():
                    st.markdown("""
                    <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); 
                                padding: 20px; border-radius: 10px; color: white;">
                    <h4 style="margin-top: 0;">R√©sultats du mod√®le MLP + SMOA</h4>
                    </div>
                    """, unsafe_allow_html=True)
                    st.text(report_smoa)
        else:
            st.info("Rapport SMOA non trouv√©")

    # Matrices de confusion c√¥te √† c√¥te
    st.markdown("---")
    st.subheader("üî• Matrices de Confusion")
    col3, col4 = st.columns(2)
    with col3:
        mlp_confusion_path = "results_mlp/confusion_matrix_mlp.png"
        if os.path.exists(mlp_confusion_path):
            st.image(mlp_confusion_path, caption="Matrice de confusion MLP", use_container_width=True)
        else:
            st.warning(f"‚ùå Fichier manquant : {mlp_confusion_path}")
    with col4:
        smoa_confusion_path = "results_smoa/confusion_matrix_smoa.png"
        if os.path.exists(smoa_confusion_path):
            st.image(smoa_confusion_path, caption="Matrice de confusion MLP + SMOA", use_container_width=True)
        else:
            st.warning(f"‚ùå Fichier manquant : {smoa_confusion_path}")

    # Courbes d'accuracy / training c√¥te √† c√¥te
    st.markdown("---")
    st.subheader("üìà Courbes d'Entra√Ænement et Accuracy")
    col5, col6 = st.columns(2)
    with col5:
        if os.path.exists(MLP_GRAPH_PATH):
            st.image(MLP_GRAPH_PATH, caption="Courbe accuracy MLP", use_container_width=True)
        else:
            st.warning(f"‚ùå Fichier manquant : {MLP_GRAPH_PATH}")
    with col6:
        if os.path.exists(SMOA_GRAPH_PATH):
            st.image(SMOA_GRAPH_PATH, caption="Courbes (loss/accuracy) MLP + SMOA", use_container_width=True)
        else:
            st.warning(f"‚ùå Fichier manquant : {SMOA_GRAPH_PATH}")

    # Fichiers complets en bas
    st.markdown("---")
    st.subheader("üìÅ Fichiers Complets")
    col7, col8 = st.columns(2)
    with col7:
        with st.expander("üìÇ Fichiers MLP (complets)"):
            render_results("", "results_mlp")
    with col8:
        with st.expander("üìÇ Fichiers SMOA (complets)"):
            render_results("", "results_smoa")

    # Hyperparam√®tres SMOA √† la fin
    st.markdown("---")
    if os.path.exists(SMOA_METRICS_PATH):
        st.subheader("‚öôÔ∏è Hyperparam√®tres Optimaux (SMOA)")
        try:
            with open(SMOA_METRICS_PATH, "r", encoding="utf-8", errors="ignore") as f:
                hp = json.load(f)
            st.json(hp)
        except Exception as e:
            st.error(f"Impossible de lire best_hyperparameters.json : {e}")

# ======================== PR√âDICTION (SMOA) ========================
elif page == "Pr√©diction (SMOA)":
    st.title("ü§ñ Pr√©dire avec le mod√®le SMOA")
    model_path = "results_smoa/best_model_smoa.pkl"
    scaler_path = "results_smoa/scaler_smoa.pkl"

    if not (os.path.exists(model_path) and os.path.exists(scaler_path)):
        st.warning("Fichiers mod√®le/scaler SMOA introuvables dans results_smoa/. Ajoute best_model_smoa.pkl et scaler_smoa.pkl.")
    else:
        try:
            model = joblib.load(model_path)
            scaler = joblib.load(scaler_path)
        except Exception as e:
            st.error(f"Erreur lors du chargement du mod√®le ou du scaler : {e}")
            model = None
            scaler = None

        if model is not None and scaler is not None:
            st.markdown("üìù **Formulaire rapide** (12 premi√®res features) pour une pr√©diction unique, puis pr√©dictions par CSV.")

            # Charger un √©chantillon de r√©f√©rence pour proposer des valeurs
            df_ref = None
            if os.path.exists(DATA_PATH):
                try:
                    df_ref = pd.read_csv(DATA_PATH).drop(columns=["Cover_Type"], errors="ignore")
                except Exception:
                    df_ref = None

            feature_list_full = list(getattr(scaler, "feature_names_in_", []))
            feature_list = feature_list_full[:12] if feature_list_full else []

            form_values = {}
            with st.form("form_pred_single"):
                cols = st.columns(2)
                for idx, col in enumerate(feature_list):
                    target_col = cols[idx % 2]
                    series = None
                    if df_ref is not None and col in df_ref.columns:
                        series = df_ref[col].dropna()

                    options = None
                    default_val = 0.0
                    if series is not None and not series.empty:
                        uniq = series.unique()
                        if len(uniq) <= 20:
                            options = sorted(pd.unique(uniq).tolist())
                        default_val = float(series.median()) if pd.api.types.is_numeric_dtype(series) else 0.0

                    with target_col:
                        if options is not None:
                            selected = st.selectbox(f"üìå {col}", options)
                            form_values[col] = selected
                        else:
                            form_values[col] = st.number_input(f"üî¢ {col}", value=default_val)

                submitted = st.form_submit_button("üéØ Pr√©dire (ligne unique)")

            if submitted:
                try:
                    all_values = form_values.copy()
                    missing_features = [f for f in feature_list_full if f not in all_values]
                    if missing_features and df_ref is not None:
                        random_row = df_ref.sample(1).iloc[0]
                        for feat in missing_features:
                            if feat in df_ref.columns:
                                all_values[feat] = random_row[feat]
                            else:
                                all_values[feat] = 0
                    row_df = pd.DataFrame([all_values])[feature_list_full]
                    row_scaled = scaler.transform(row_df)
                    pred = model.predict(row_scaled)[0]
                    st.success(f"Classe pr√©dite : **{pred}**")
                except Exception as e:
                    st.error(f"Erreur lors de la pr√©diction via formulaire : {e}")

            st.markdown("---")
            st.markdown("üì§ **Pr√©dictions par CSV** : Charge un CSV avec les m√™mes colonnes d'entra√Ænement (sans la cible). Si 'Cover_Type' est pr√©sent, il sera ignor√©.")
            uploaded = st.file_uploader("üìÅ Choisir un fichier CSV", type=["csv"])

            df_pred = None
            if uploaded is not None:
                try:
                    df_pred = pd.read_csv(uploaded)
                except Exception as e:
                    st.error(f"Impossible de lire le CSV : {e}")

            if df_pred is None and os.path.exists(DATA_PATH):
                st.info("‚úÖ Aucun fichier charg√©, utilisation des 50 premi√®res lignes du dataset local (sans la cible).")
                try:
                    df_pred = pd.read_csv(DATA_PATH).head(50)
                except Exception as e:
                    st.error(f"Impossible de lire {DATA_PATH} : {e}")

            if df_pred is not None:
                if "Cover_Type" in df_pred.columns:
                    df_pred = df_pred.drop(columns=["Cover_Type"])

                st.write("üëÅÔ∏è **Pr√©visualisation des donn√©es d'entr√©e :**")
                st.dataframe(df_pred.head(10))

                try:
                    if hasattr(scaler, "feature_names_in_"):
                        missing = [c for c in scaler.feature_names_in_ if c not in df_pred.columns]
                        if missing:
                            st.error(f"Colonnes manquantes pour la pr√©diction : {missing}")
                        else:
                            X = df_pred[scaler.feature_names_in_]
                    else:
                        X = df_pred

                    X_scaled = scaler.transform(X)
                    preds = model.predict(X_scaled)
                    st.success("‚úÖ Pr√©dictions g√©n√©r√©es avec succ√®s!")
                    out = df_pred.copy()
                    out["prediction"] = preds
                    st.dataframe(out.head(20))

                    # R√©partition des classes pr√©dites
                    st.subheader("üìä R√©partition des classes pr√©dites")
                    counts = pd.Series(preds).value_counts().sort_index()
                    st.bar_chart(counts)

                    # T√©l√©chargement
                    csv_bytes = out.to_csv(index=False).encode("utf-8")
                    st.download_button("‚¨áÔ∏è T√©l√©charger les pr√©dictions (CSV)", csv_bytes, "predictions_smoa.csv", "text/csv")
                except Exception as e:
                    st.error(f"Erreur lors de la pr√©diction : {e}")

# ======================== PAGE ALGO (explications) ========================
elif page == "Algo (explications)":
    st.title("üß¨ Algorithmes : MLP et SMOA")
    st.markdown("""
    Cette page explique le fonctionnement des deux approches utilis√©es :
    - üß† **MLP** (Multilayer Perceptron) pour la classification.
    - üî¨ **SMOA** (optimisation m√©ta-heuristique) pour chercher de bons hyperparam√®tres du MLP.

    √âtapes cl√©s (hors pr√©traitement et sauvegardes) :
    - 1Ô∏è‚É£ D√©finition d'une fonction objectif qui, √† partir d'un vecteur de param√®tres, construit un MLP et √©value sa performance (accuracy de validation).
    - 2Ô∏è‚É£ SMOA explore l'espace des hyperparam√®tres du MLP, en combinant mouvements vers le meilleur, recherche locale guid√©e par des param√®tres (magn√©tisme, rayon de senteur), et arr√™t anticip√©.
    - 3Ô∏è‚É£ Une fois les meilleurs hyperparam√®tres trouv√©s, on entra√Æne le MLP final sur plusieurs √©poques en enregistrant l'√©volution de l'accuracy (courbe d'am√©lioration).
    """)

    st.subheader("üíª Code de r√©f√©rence (SMOA + MLP)")
    code_str = """# mlp_smoa_forest_cover_colab.py
"""
    # Ins√©rer le code fourni (corrig√© minimalement pour l'affichage)
    algo_code = '''# mlp_smoa_forest_cover_colab.py
"""
MLP + SMOA optimisation sur le dataset Forest Cover Type
Colonne cible : Cover_Type (7 classes)
Split : 80% train / 20% test
Sauvegarde dans results_smoa/
Compatible Colab avec t√©l√©chargement depuis Kaggle
"""

import os
import json
import joblib
import inspect
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shutil

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

plt.style.use("default")
sns.set_palette("husl")
plt.rcParams["figure.figsize"] = [12, 8]

RND = 42
DATA_PATH = "data/covtype.csv"

# ---------------------------
# SMOA (coeur de l'algo)
# ---------------------------
class SMOA:
    def __init__(self, obj_fn, dim, pop_size=8, lb=-3.0, ub=3.0, max_iter=15,
                 mag_strength=0.8, scent_radius=0.5, decay_rate=0.05,
                 sinus_amp=0.1, sinus_freq=0.05, diversity_threshold=1e-4,
                 early_stop=6, seed=None):
        self.obj = obj_fn
        self.dim = dim
        self.pop_size = pop_size
        self.lb = np.ones(dim) * lb if np.isscalar(lb) else np.array(lb)
        self.ub = np.ones(dim) * ub if np.isscalar(ub) else np.array(ub)
        self.max_iter = max_iter
        self.rng = np.random.default_rng(seed)
        self.mag_strength0 = mag_strength
        self.scent_radius0 = scent_radius
        self.decay_rate = decay_rate
        self.sinus_amp = sinus_amp
        self.sinus_freq = sinus_freq
        self.diversity_threshold = diversity_threshold
        self.early_stop = early_stop

    def clamp(self, pop):
        return np.clip(pop, self.lb, self.ub)

    def population_diversity(self, pop):
        return np.mean(np.std(pop, axis=0))

    def adapt_parameters(self, t):
        exp_term = np.exp(-self.decay_rate * t)
        mag = self.mag_strength0 * exp_term + self.sinus_amp * np.sin(2 * np.pi * self.sinus_freq * t)
        scent = self.scent_radius0 * exp_term * (0.7 + 0.3 * np.sin(2 * np.pi * self.sinus_freq * t))
        return max(1e-6, mag), max(1e-6, scent)

    def magnet_move(self, x, best, mag_strength):
        direction = best - x
        dist = np.linalg.norm(direction)
        if dist > 1e-12:
            unit = direction / dist
        else:
            unit = self.rng.normal(size=self.dim)
            unit /= np.linalg.norm(unit)
        rand = self.rng.normal(size=self.dim)
        step = mag_strength * (unit * (0.5 + self.rng.random()) + 0.1 * rand)
        return x + step

    def scent_local_search(self, x, scent_radius, n_samples=3):
        best_x = x.copy()
        best_val, _ = self.obj(best_x)
        for _ in range(n_samples):
            direction = self.rng.normal(size=self.dim)
            nrm = np.linalg.norm(direction)
            if nrm > 0:
                direction = direction / nrm
            r = (self.rng.random() ** (1 / self.dim))
            candidate = x + direction * r * scent_radius * (self.ub - self.lb)
            candidate = self.clamp(candidate)
            val, _ = self.obj(candidate)
            if val < best_val:
                best_x = candidate.copy()
                best_val = val
        return best_x, best_val

    def run(self, verbose=True):
        pop = self.rng.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))
        fitness = np.array([self.obj(x)[0] for x in pop])
        best_idx = np.argmin(fitness)
        gbest = pop[best_idx].copy()
        gbest_val, gbest_metrics = self.obj(gbest)
        no_improve = 0
        history = {"best": [gbest_val], "mean": [np.mean(fitness)], "best_params": []}

        for t in range(self.max_iter):
            mag_strength, scent_radius = self.adapt_parameters(t)
            div = self.population_diversity(pop)
            if div < self.diversity_threshold:
                mag_strength *= 0.5
                scent_radius *= 1.5

            new_pop = pop.copy()
            new_fit = fitness.copy()
            for i in range(self.pop_size):
                x = pop[i]
                moved = self.magnet_move(x, gbest, mag_strength)
                local_x, local_f = self.scent_local_search(moved, scent_radius)
                new_pop[i] = local_x
                new_fit[i] = local_f
                if local_f < gbest_val:
                    gbest = local_x.copy()
                    gbest_val = local_f
                    gbest_metrics = self.obj(gbest)[1]
                    no_improve = 0

            pop = new_pop
            fitness = new_fit
            if fitness.min() < gbest_val:
                best_idx = np.argmin(fitness)
                gbest = pop[best_idx].copy()
                gbest_val = fitness[best_idx]
                gbest_metrics = self.obj(gbest)[1]
                no_improve = 0
            else:
                no_improve += 1

            history["best"].append(gbest_val)
            history["mean"].append(np.mean(fitness))
            history["best_params"].append(gbest_metrics.get("params", {}))

            if no_improve >= self.early_stop:
                break

        return gbest, gbest_val, gbest_metrics, history


# ---------------------------
# Objectif pour MLP
# ---------------------------
def make_objective(X_train, X_val, y_train, y_val, n_epochs_eval=10):
    activations = ["relu", "tanh", "logistic"]
    solvers = ["adam", "sgd"]

    def obj_fn(vec):
        vec = np.asarray(vec)
        h1 = max(16, int(round(abs(vec[0]) * 240)) + 16)
        h2 = max(8,  int(round(abs(vec[1]) * 120)) + 8)
        hidden_layers = (h1, h2)
        act_idx = int(abs(vec[2]) * len(activations)) % len(activations)
        activation = activations[act_idx]
        solver_idx = int(abs(vec[3]) * len(solvers)) % len(solvers)
        solver = solvers[solver_idx]
        alpha = 10 ** np.clip(vec[4], -6, 0)
        max_iter = int(50 + np.clip(vec[5], 0, 1) * 350)
        lr_init = 10 ** np.clip(vec[6], -5, -1)
        beta_1 = np.clip(vec[7], 0.8, 0.999)
        beta_2 = np.clip(vec[8], 0.9, 0.9999)
        tol = 10 ** np.clip(vec[9], -7, -3)

        params = {
            "hidden_layer_sizes": hidden_layers,
            "activation": activation,
            "solver": solver,
            "alpha": alpha,
            "max_iter": max_iter,
            "learning_rate_init": lr_init,
            "beta_1": beta_1,
            "beta_2": beta_2,
            "tol": tol,
            "random_state": 42,
            "verbose": False,
        }

        sig = inspect.signature(MLPClassifier.__init__)
        accepted = {k for k in sig.parameters.keys() if k != "self"}
        filtered = {k: v for k, v in params.items() if k in accepted}

        try:
            model = MLPClassifier(**filtered)
            classes = np.unique(y_train)
            for _ in range(n_epochs_eval):
                idx = np.random.permutation(len(X_train))
                model.partial_fit(X_train[idx], y_train[idx], classes=classes)
            val_acc = model.score(X_val, y_val)
            loss = 1.0 - val_acc
            metrics = {"params": params, "val_acc": val_acc}
        except Exception as e:
            loss = 1.0
            metrics = {"error": str(e)}

        return loss, metrics

    return obj_fn

'''
    st.code(algo_code, language="python")

    # ======================== EXPLICATIONS MATH√âMATIQUES ========================
    st.subheader("üìê Explications math√©matiques et fonctionnement d√©taill√©")
    
    st.markdown("""
    ### 1. **Initialisation de la population**
    
    La population initiale est g√©n√©r√©e al√©atoirement dans l'espace de recherche :
    
    $$\\mathbf{x}_i^{(0)} \\sim \\text{Uniform}(\\mathbf{lb}, \\mathbf{ub}), \\quad i = 1, 2, \\ldots, N_{pop}$$
    
    o√π $\\mathbf{lb}$ et $\\mathbf{ub}$ sont les bornes inf√©rieure et sup√©rieure de l'espace de recherche, 
    et $N_{pop}$ est la taille de la population.
    
    ---
    
    ### 2. **Param√®tres adaptatifs (magn√©tisme et rayon de senteur)**
    
    Les param√®tres de contr√¥le √©voluent au cours des it√©rations pour √©quilibrer exploration et exploitation :
    
    $$\\text{mag}(t) = \\text{mag}_0 \\cdot e^{-\\text{decay\\_rate} \\cdot t} + \\text{sinus\\_amp} \\cdot \\sin(2\\pi \\cdot \\text{sinus\\_freq} \\cdot t)$$
    
    $$\\text{scent}(t) = \\text{scent}_0 \\cdot e^{-\\text{decay\\_rate} \\cdot t} \\cdot \\left(0.7 + 0.3 \\cdot \\sin(2\\pi \\cdot \\text{sinus\\_freq} \\cdot t)\\right)$$
    
    **Interpr√©tation** :
    - Le terme exponentiel $e^{-\\text{decay\\_rate} \\cdot t}$ r√©duit progressivement les param√®tres (exploitation croissante)
    - Le terme sinuso√Ødal ajoute des oscillations pour √©chapper aux minima locaux (exploration)
    
    ---
    
    ### 3. **Mouvement magn√©tique (vers le meilleur)**
    
    Chaque particule se d√©place vers le meilleur point trouv√© ($\\mathbf{g}_{best}$) avec une att√©nuation al√©atoire :
    
    $$\\mathbf{d} = \\mathbf{g}_{best} - \\mathbf{x}_i$$
    
    $$\\mathbf{u} = \\begin{cases} \\frac{\\mathbf{d}}{\\|\\mathbf{d}\\|_2} & \\text{si } \\|\\mathbf{d}\\| > 10^{-12} \\\\ \\text{Normal}(0, 1) & \\text{sinon} \\end{cases}$$
    
    $$\\mathbf{x}_i^{\\text{moved}} = \\mathbf{x}_i + \\text{mag}(t) \\cdot \\left(\\mathbf{u} \\cdot (0.5 + r_1) + 0.1 \\cdot \\boldsymbol{\\epsilon}\\right)$$
    
    o√π $r_1 \\sim \\text{Uniform}(0, 1)$ et $\\boldsymbol{\\epsilon} \\sim \\text{Normal}(0, 1)$.
    
    **Interpr√©tation** : La particule se rapproche du meilleur avec du bruit stochastique pour explorer localement.
    
    ---
    
    ### 4. **Recherche locale guid√©e (rayon de senteur)**
    
    Autour de la particule d√©plac√©e, on explore le voisinage pour trouver une meilleure solution :
    
    $$\\mathbf{c}_j = \\mathbf{x}_i^{\\text{moved}} + \\mathbf{v} \\cdot r \\cdot \\text{scent}(t) \\cdot (\\mathbf{ub} - \\mathbf{lb})$$
    
    o√π :
    - $\\mathbf{v} \\sim \\text{Normal}(0, 1)$ (direction al√©atoire normalis√©e)
    - $r \\sim \\text{Uniform}(0, 1)^{1/d}$ (rayon adaptatif √† la dimension)
    - $d$ est la dimension de l'espace
    
    Pour chaque candidat $\\mathbf{c}_j$, on √©value la fonction objectif et on garde le meilleur localement.
    
    **Interpr√©tation** : On explore un rayon sph√©rique autour de la particule pour affiner la solution.
    
    ---
    
    ### 5. **Mise √† jour de la meilleure solution globale**
    
    √Ä chaque it√©ration, on met √† jour le meilleur point trouv√© :
    
    $$f(\\mathbf{g}_{best}^{\\text{new}}) \\leq f(\\mathbf{g}_{best}^{\\text{old}})$$
    
    Si la diversit√© de la population devient faible, on augmente l'exploration :
    
    $$\\text{Diversit√©} = \\frac{1}{d} \\sum_{k=1}^{d} \\sigma_k < \\text{diversity\\_threshold}$$
    
    Alors :
    $$\\text{mag}(t) \\leftarrow \\text{mag}(t) \\times 0.5, \\quad \\text{scent}(t) \\leftarrow \\text{scent}(t) \\times 1.5$$
    
    ---
    
    ### 6. **Crit√®res d'arr√™t**
    
    L'algorithme s'arr√™te si :
    - **Nombre d'it√©rations atteint** : $t \\geq t_{\\max}$
    - **Pas d'am√©lioration** : Aucune meilleure solution pendant $\\text{early\\_stop}$ it√©rations
    
    ---
    
    ### 7. **Fonction objectif pour l'optimisation du MLP**
    
    √âtant donn√© un vecteur de param√®tres $\\mathbf{vec} \\in \\mathbb{R}^{10}$, on construit un MLP et √©value sa performance :
    
    $$\\text{Loss}(\\mathbf{vec}) = 1 - \\text{Accuracy}_{validation}(\\text{MLP}(\\mathbf{vec}))$$
    
    Les param√®tres du MLP sont d√©cod√©s √† partir de $\\mathbf{vec}$ :
    
    | Indice | Param√®tre | Formule |
    |--------|-----------|---------|
    | 0 | $h_1$ (couche 1) | $\\max(16, \\lfloor \\|\\text{vec}[0]\\| \\times 240 \\rfloor + 16)$ |
    | 1 | $h_2$ (couche 2) | $\\max(8, \\lfloor \\|\\text{vec}[1]\\| \\times 120 \\rfloor + 8)$ |
    | 2 | activation | $\\{\\text{relu}, \\text{tanh}, \\text{logistic}\\}[\\lfloor \\|\\text{vec}[2]\\| \\cdot 3 \\rfloor]$ |
    | 3 | solver | $\\{\\text{adam}, \\text{sgd}\\}[\\lfloor \\|\\text{vec}[3]\\| \\cdot 2 \\rfloor]$ |
    | 4 | alpha (L2) | $10^{\\text{clip}(\\text{vec}[4], -6, 0)}$ |
    | 5 | max_iter | $\\lfloor 50 + \\text{clip}(\\text{vec}[5], 0, 1) \\times 350 \\rfloor$ |
    | 6 | learning_rate | $10^{\\text{clip}(\\text{vec}[6], -5, -1)}$ |
    | 7 | $\\beta_1$ (momentum) | $\\text{clip}(\\text{vec}[7], 0.8, 0.999)$ |
    | 8 | $\\beta_2$ (RMSprop) | $\\text{clip}(\\text{vec}[8], 0.9, 0.9999)$ |
    | 9 | tolerance | $10^{\\text{clip}(\\text{vec}[9], -7, -3)}$ |
    
    ---
    
    ### **R√©sum√© du processus (step-by-step)**
    
    1. **Initialisation** : G√©n√©rer une population al√©atoire dans l'espace des hyperparam√®tres
    2. **√âvaluation initiale** : Calculer la qualit√© (accuracy) pour chaque particule
    3. **Boucle principale** (jusqu'√† convergence) :
        - **Adaptation** : Mettre √† jour les param√®tres $\\text{mag}(t)$ et $\\text{scent}(t)$
        - **Mouvement magn√©tique** : D√©placer chaque particule vers $\\mathbf{g}_{best}$
        - **Recherche locale** : Explorer le rayon de senteur autour de la nouvelle position
        - **Mise √† jour globale** : Mettre √† jour $\\mathbf{g}_{best}$ si am√©lioration
        - **V√©rification de la diversit√©** : Si trop faible, augmenter exploration
    4. **Retour** : Les meilleurs hyperparam√®tres trouv√©s
    5. **Entra√Ænement final** : Entra√Æner le MLP avec ces hyperparam√®tres sur tout l'ensemble d'entra√Ænement
    
    ---
    
    **Avantages de SMOA** :
    - ‚úÖ √âquilibre exploration/exploitation dynamique
    - ‚úÖ Pas de gradient requis (m√©ta-heuristique)
    - ‚úÖ Adapt√© √† la recherche d'hyperparam√®tres discrets et continus
    - ‚úÖ Arr√™t anticip√© pour √©conomiser les ressources
    """)

    st.markdown("""
    En pratique dans cette application, nous affichons uniquement les r√©sultats sauvegard√©s. 
    Ajoute tes fichiers dans `results_smoa/` pour voir les sorties correspondantes dans l'onglet Comparaison.
    """)

# ======================== FOOTER ========================
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; font-size: 14px;">
üå≤ <b>Application Streamlit</b> ‚Äì Affichage et Analyse | üß† Comparaison MLP vs MLP + SMOA | üìä 2025
</div>
""", unsafe_allow_html=True)